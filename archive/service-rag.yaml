name: rag-chatbot
message: Quickstart to serve Llama 3.1 model with RAG.
# description: RAG chatbot
# tags:
#   - RAG
#   - LLM
resources:
  cluster: snu-eng-dgx
  preset: a100-1
image: quay.io/vessl-ai/torch:2.3.1-cuda12.1-r5  # PyTorch 2.3.1 (CUDA 12.1)
import:
  /app: git://github.com/pikaybh/RAG-vllm-RA.git
run:
  - command: |  # Install some dependencies for LLM acceleration those have conflicts with common libraries
      apt update && apt install -y libgl1
      pip install autoawq>=0.2.6
      pip install vllm>=0.6.3
      pip install flash-attn>=2.6.3
    workdir: /app
  - command: |
      vllm serve $MODEL_NAME --dtype auto --gpu-memory-utilization 0.8 &
    workdir: /app
  - command: |
      pip install -r requirements.txt
    workdir: /app
  - command: |
      python app_gradio.py --llm-host http://localhost:8000/v1
    workdir: /app
ports:
  - name: gradio
    type: http
    port: 7860
  - name: vllm
    type: http
    port: 8000
  - name: metrics
    type: http
    port: 5000
service:
  autoscaling:
    max: 2
    metric: cpu
    min: 1
    target: 50
  monitoring:
    - port: 5000
      path: /metrics
  expose: 7860
env:
  MODEL_NAME: casperhansen/llama-3-8b-instruct-awq
